In this work, in the context of this NLP, we investigate the so-called authorship attribution problem in three versions: 

(1) given two texts T1 and T2, are both generated by the same method or not? 
(2) is the given text T written by a human or machine? 
(3) given a text T and k candidate neural methods, can we single out the method (among k alternatives) that generated T? 

Against one humanwritten and eight machine-generated texts (i.e., CTRL, GPT, GPT2, GROVER, XLM, XLNET, PPLM, FAIR). 

We empirically experiment with the performance of various models in three problems. By and large, we find that most generators still generate texts significantly different from human-written ones, thereby making three problems easier to solve.
